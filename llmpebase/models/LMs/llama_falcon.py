"""
Getting the pre-trained LLaMA model for inference.
"""

from typing import List

import torch
from transformers import LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM
import tensor_parallel as tp

from vgbase.utils.folder_utils import directory_contains_subfolder

from llmpebase.models.LMs.prompt_utlis import batch_split


class LLaMARequest(object):
    """A class to forward the LLaMA model."""

    def __init__(self, model_config: dict, envs_config: dict) -> None:
        self.model, self.tokenizer = self.load_model(model_config, envs_config)

        self.model.eval()

    def load_model(self, model_config: dict, envs_config: dict):
        """loading the llama models."""
        model_name = model_config["model_name"]
        checkpoint_dir = model_name
        if "pretrained_models_dir" in model_config and directory_contains_subfolder(
            model_config["pretrained_models_dir"], model_name
        ):
            checkpoint_dir = model_config["pretrained_models_dir"]

        model_type = model_config["model_type"]
        assert model_type in ["llama", "falcon"]

        tokenizer = LlamaTokenizer.from_pretrained(
            checkpoint_dir,
            use_fast=False,
            padding_side="left",
        )
        tokenizer.pad_token_id = (
            0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id
        )
        tokenizer.bos_token_id = 1

        n_gpus = envs_config["world_size"]
        if model_type == "llama":
            # we use tensor parallel for loading llama
            model = LlamaForCausalLM.from_pretrained(
                checkpoint_dir,
                torch_dtype=torch.float16,
                device_map="auto",
                offload_folder="offload",
            )
            model = tp.tensor_parallel(model, [i for i in range(n_gpus)])
        else:
            model = AutoModelForCausalLM.from_pretrained(
                checkpoint_dir,
                torch_dtype=torch.float16,
                trust_remote_code=True,
            )
        return model, tokenizer

    def create_format_prompt(self, instruction: str, prompts: List[str]):
        """Creating prompts for the LLaMA models."""
        instruction = f"{instruction}\n\n"
        format_prompt = "\n".join(prompts)
        return format_prompt

    def get_tokens_input(self, prompts):
        """Getting the tokens of prompts"""
        input_tokens = self.tokenizer.batch_encode_plus(
            prompts, return_tensors="pt", padding=True
        )
        for t in input_tokens:
            if torch.is_tensor(input_tokens[t]):
                input_tokens[t] = input_tokens[t].to("cuda")

        return input_tokens

    def forward(self, format_prompts: List[str], batch_size: int = 8):
        """Forwarding the model for prompts.

        :param format_prompts: A `List` containing a list of formatted
         prompts generated by the function 'create_format_prompt'.
        """

        answers = []
        for batch_input in batch_split(format_prompts, batch_size):
            encode_inputs = self.get_tokens_input(batch_input)
            outputs = self.model.generate(**encode_inputs, max_new_tokens=1)

            answers.extend(
                self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
            )
        answers = [answer[-1] for answer in answers]
        return answers

"""
The implementation of base model for LMs.
"""
import re
from typing import Union, List

import torch


class BaseLMRequest(torch.nn.Module):
    """The basic request model for large language model."""

    def __init__(self, model_config: dict):
        super().__init__()

        # Get the model information
        self.model_config = model_config
        self.model_name = model_config["model_name"]

        # Configuration for the generation
        self.generation_config = {}

        # Basic components to record the resource used
        self.num_requests = 0
        self.num_prompt_tokens = []
        self.num_response_tokens = []

        self.configuration()

    def configuration(self):
        """Configuration of the model."""
        # 1. Temperature is a parameter that controls the "creativity" or randomness
        # of the text generated by GPT-3. A higher temperature (e.g., 0.7) results in
        # more diverse and creative output, while a lower temperature (e.g., 0.2) makes
        # the output more deterministic and focused.
        # In practice, temperature affects the probability distribution over the
        # possible tokens at each step of the generation process. A temperature of 0
        # would make the model completely deterministic, always choosing the most
        # likely token.
        # 2. Top P ranges from 0 to 1 (default), and a lower Top P means the model
        # samples from a narrower selection of words. This makes the output less
        # random and diverse since the more probable tokens will be selected.
        self.generation_config = {
            "temperature": 0.7,
            "max_tokens": 500,
            "top_p": 0.7,
            "stop": None,
        }

    def create_format_input(self, user_prompt, **kwargs):
        """Creating the format input received by the"""
        raise NotImplementedError("'create_format_input' has not been implemented yet.")

    def forward(
        self,
        input_request: Union[List[dict], str] = None,
        user_prompt: str = None,
        per_request_responses: int = 1,
        **kwargs,
    ):
        """Performing request once.

        :param request_input: The input of the model to perform a request. As this is the
         directly input, the type of this argument should depends on which model is used
         to perform the request.
         For ChatGPTs, the request_input should be the `List[dict]` containing multiple terms
         For Llama, the request_input should be the a string

        :param user_prompt: A `string` containing the prompt defined by the user, otherwise,
         it maybe not be the desired input of the model to perform a request. Thus, further
         processing should be implemented when necessary.

        :param per_request_responses: A `int` showing how many responses will be returned by
         the model.
        """
        raise NotImplementedError("'forward' has not been implemented yet.")

    def read_response_contents(self, responses: list):
        """Read main contents from the obtained responses."""
        raise NotImplementedError("'extract_answers' has not been implemented yet.")

    def count_tokens(self, responses: list):
        """Count answers from the obtained responses."""
        raise NotImplementedError("'extract_tokens' has not been implemented yet.")

    def is_limit_request(self):
        """Whether the request model has limited request rate."""

        return False

    @generation_config.setter
    def generation_config(self, new_config):
        """Update the generation config with the given kwargs."""
        self.generation_config.update(new_config)
